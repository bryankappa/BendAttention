# BendAttention
Attention in LLMs utulizing fast object allocation inference utulizing all of parallel hardware in utulizing GPU
